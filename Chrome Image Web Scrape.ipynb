{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0bb11216-25e5-4a8a-98e4-9113e7ace26d",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "from PIL import Image\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "from urllib.parse import quote_plus\n",
    "import re\n",
    "import ctypes\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException, ElementNotInteractableException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Prevent sleep mode while the script is running\n",
    "ctypes.windll.kernel32.SetThreadExecutionState(0x80000002)\n",
    "\n",
    "# Download the driver from the ChromeDriver website for the relevant OS i.e. MAC, Windows, etc.\n",
    "PATH = r'C:/path/to/your/chromedriver.exe'\n",
    "service = Service(executable_path=PATH)\n",
    "\n",
    "# Initialize the WebDriver with the Service object\n",
    "wd = webdriver.Chrome(service=service)\n",
    "\n",
    "def get_images_from_google(wd, delay, max_images, url):\n",
    "    def scroll_down(wd):\n",
    "        wd.execute_script(\"window.scrollBy(0, 200);\")  # Scroll slightly to move past suggestions\n",
    "        time.sleep(1)  # Allow time for suggestions to be cleared\n",
    "\n",
    "    wd.get(url)\n",
    "    scroll_down(wd)  # Scroll immediately after the search page loads\n",
    "\n",
    "    image_urls = set()\n",
    "    skips = 0\n",
    "    while len(image_urls) + skips < max_images:\n",
    "        # Find the image thumbnails on the page\n",
    "        thumbnails = wd.find_elements(By.CLASS_NAME, \"mNsIhb\")\n",
    "\n",
    "        for img in thumbnails[len(image_urls) + skips:max_images]:\n",
    "            try:\n",
    "                # Ensure the thumbnail is clickable before clicking\n",
    "                WebDriverWait(wd, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"mNsIhb\")))\n",
    "\n",
    "                # Scroll the image into view to ensure it's interactable\n",
    "                wd.execute_script(\"arguments[0].scrollIntoView(true);\", img)\n",
    "\n",
    "                img.click()  # Click the image\n",
    "                time.sleep(delay)\n",
    "\n",
    "            except (ElementClickInterceptedException, TimeoutException, ElementNotInteractableException) as e:\n",
    "                # Log or print the error and skip the problematic element\n",
    "                print(f\"Error clicking on thumbnail: {e}\")\n",
    "                skips += 1\n",
    "                continue\n",
    "\n",
    "            # Now grab the large image\n",
    "            images = wd.find_elements(By.CSS_SELECTOR, \"img.sFlh5c:not(.GKS7s):not(.bqW4cb)\")\n",
    "            for image in images:\n",
    "                if image.get_attribute('src') in image_urls:\n",
    "                    max_images += 1  # Adjust max_images to continue gathering more unique images\n",
    "                    skips += 1\n",
    "                    break\n",
    "\n",
    "                if image.get_attribute('src') and 'http' in image.get_attribute('src'):\n",
    "                    image_urls.add(image.get_attribute('src'))\n",
    "                    \n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # Break the loop if no new thumbnails are found\n",
    "        if len(thumbnails) == len(image_urls) + skips:\n",
    "            print(f\"No more thumbnails found. Proceed....\")\n",
    "            break\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "\n",
    "def download_image(down_path, url, file_name, image_type='JPEG', verbose=True):\n",
    "    try:\n",
    "        time = dt.now()\n",
    "        curr_time = time.strftime('%H:%M:%S')\n",
    "        # Content of the image will be a url\n",
    "        img_content = requests.get(url).content\n",
    "        # Get the bytes IO of the image\n",
    "        img_file = io.BytesIO(img_content)\n",
    "        # Stores the file in memory and convert to image file using Pillow\n",
    "        image = Image.open(img_file)\n",
    "\n",
    "        # If image is in WebP format, convert it to JPEG or PNG\n",
    "        if image.format =! 'WEBP':\n",
    "            # Save the image normally for non-WebP formats\n",
    "            file_pth = down_path + file_name\n",
    "            image.save(file_pth, image_type)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'The image: {file_pth} downloaded successfully at {curr_time}.')\n",
    "    except Exception as e:\n",
    "        print(f'Unable to download image from Google Photos')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ask user for the list of foods they want to search, separated by commas\n",
    "    food_list = input(\"Enter the names of the foods you want to search for, separated by commas: \").split(',')\n",
    "    food_list = [food.strip() for food in food_list]  # Remove any leading/trailing whitespace\n",
    "\n",
    "    # Loop to ensure valid integer input for the maximum number of images to scrape\n",
    "    while True:\n",
    "        try:\n",
    "            max_images = int(input(\"Enter the maximum number of images to scrape for each food item: \"))\n",
    "            break  # Exit the loop if a valid integer is entered\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer for the number of images.\")\n",
    "\n",
    "    # Get the current date in the format YYYY-MM-DD\n",
    "    current_date = dt.now().strftime('%Y-%m-%d')\n",
    "    base_directory = f'C:/Users/YourUserName/Desktop/Food AI/images/Malaysian Dish/{current_date}/'\n",
    "\n",
    "    # Make the base directory for the current date if it doesn't exist\n",
    "    if not os.path.exists(base_directory):\n",
    "        print(f'Making base directory for the current date: {base_directory}')\n",
    "        os.makedirs(base_directory)\n",
    "\n",
    "    for food_name in food_list:\n",
    "        # URL encode the food name for the Google search URL\n",
    "        search_query = quote_plus(food_name)\n",
    "        google_url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
    "\n",
    "        # Directory to save the images, named after the food\n",
    "        sanitized_name = re.sub(r'[\\\\/:*?\"<>|]', '', food_name)\n",
    "        save_directory = os.path.join(base_directory, sanitized_name)\n",
    "\n",
    "        # Make the directory for the specific food if it doesn't exist\n",
    "        if not os.path.exists(save_directory):\n",
    "            print(f'Making directory for {food_name}: {save_directory}')\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        # Scrape images from Google\n",
    "        urls = get_images_from_google(wd, delay=0.5, max_images=max_images, url=google_url)\n",
    "\n",
    "        # Download the images\n",
    "        for i, url in enumerate(urls):\n",
    "            download_image(down_path=save_directory + '/', \n",
    "                           url=url, \n",
    "                           file_name=f\"{sanitized_name}_\" + str(i+1) + '.jpg',\n",
    "                           verbose=True)\n",
    "\n",
    "    # Close the webdriver\n",
    "    wd.quit()\n",
    "\n",
    "    # Allow sleep mode again after script execution\n",
    "    ctypes.windll.kernel32.SetThreadExecutionState(0x80000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d708f48-08a3-4b5f-9fd5-ab77abe3dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate Detection\n",
    "\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import imagehash\n",
    "import shutil\n",
    "\n",
    "def find_duplicates(directory_path, hamming_threshold):\n",
    "    image_hashes = {}  # Dictionary to store image hashes\n",
    "    duplicates = []  # List to store groups of duplicate images\n",
    "\n",
    "    # Calculate image hashes for each image in the directory\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        # Ignore the 'duplicates' folder\n",
    "        if 'duplicates' in root or 'collage_images' in root or 'text_detected' in root:\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    image = Image.open(image_path)\n",
    "                    image_hash = imagehash.phash(image)\n",
    "\n",
    "                    # Check for duplicates using Hamming distance\n",
    "                    found_duplicate = False\n",
    "                    for existing_hash, existing_paths in image_hashes.items():\n",
    "                        hamming_distance = image_hash - existing_hash\n",
    "                        if hamming_distance <= hamming_threshold:\n",
    "                            existing_paths.append(image_path)  # Add to existing group of duplicates\n",
    "                            found_duplicate = True\n",
    "                            break\n",
    "\n",
    "                    # Only add to the dictionary if no duplicate was found\n",
    "                    if not found_duplicate:\n",
    "                        image_hashes[image_hash] = [image_path]\n",
    "\n",
    "                except UnidentifiedImageError:\n",
    "                    print(f\"Cannot identify image file: {image_path}. Deleting file.\")\n",
    "                    os.remove(image_path)  # Automatically delete the file\n",
    "\n",
    "    print(\"Duplicate detection result:\")\n",
    "    duplicates = [paths for paths in image_hashes.values() if len(paths) > 1]\n",
    "    return duplicates\n",
    "\n",
    "def calculate_similarity(hamming_distance, max_distance):\n",
    "    similarity_percentage = (1 - (hamming_distance / max_distance)) * 100\n",
    "    return round(similarity_percentage, 2)\n",
    "\n",
    "def move_and_rename_duplicates(duplicates):\n",
    "    pair_counter = {}  # To track pair counts for each food folder\n",
    "    grouped_new_paths = []  # Store new paths grouped by duplicate groups\n",
    "\n",
    "    for group in duplicates:\n",
    "        food_folder = os.path.dirname(group[0])\n",
    "        food_name = os.path.basename(food_folder)  # Extract the food folder name\n",
    "        duplicates_folder = os.path.join(food_folder, 'duplicates')\n",
    "\n",
    "        # Ensure the duplicates folder exists\n",
    "        if not os.path.exists(duplicates_folder):\n",
    "            os.makedirs(duplicates_folder)\n",
    "            print(f\"Created 'duplicates' folder at: {duplicates_folder}\")\n",
    "\n",
    "        # Initialize the pair counter for this food item folder\n",
    "        if food_name not in pair_counter:\n",
    "            pair_counter[food_name] = 1\n",
    "\n",
    "        new_group_paths = []  # To store the paths of the current group\n",
    "\n",
    "        # Move and rename each duplicate in the group\n",
    "        pair_number = pair_counter[food_name]\n",
    "        for idx, dup in enumerate(group):\n",
    "            file_name = f\"{food_name}_DuplicateGroup{pair_number}_{idx + 1}{os.path.splitext(dup)[1]}\"\n",
    "            dest_path = os.path.join(duplicates_folder, file_name)\n",
    "            shutil.move(dup, dest_path)\n",
    "            print(f\"Moved and renamed {dup} to {file_name}\")\n",
    "            new_group_paths.append(dest_path)  # Add new path to the current group\n",
    "\n",
    "        # Store the grouped paths of this duplicate group\n",
    "        grouped_new_paths.append(new_group_paths)\n",
    "\n",
    "        # Increment the pair number for the next group in the same folder\n",
    "        pair_counter[food_name] += 1\n",
    "\n",
    "    return grouped_new_paths\n",
    "\n",
    "def delete_smaller_duplicates_and_move_back(grouped_new_paths):\n",
    "    choice = input(f\"Do you want to delete all duplicates except the largest file? (y/n): \").strip().lower()\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        for group in grouped_new_paths:\n",
    "            # Sort the duplicates by file size\n",
    "            group_sorted_by_size = []\n",
    "            for file in group:\n",
    "                try:\n",
    "                    # Attempt to get the file size and add to sorted list\n",
    "                    size = os.path.getsize(file)\n",
    "                    group_sorted_by_size.append((file, size))\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {file}. Skipping.\")\n",
    "            \n",
    "            if not group_sorted_by_size:\n",
    "                # If no valid files are found, skip the group\n",
    "                print(\"No valid files found in this group. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Sort by file size in descending order\n",
    "            group_sorted_by_size.sort(key=lambda x: x[1], reverse=True)\n",
    "            largest_file = group_sorted_by_size[0][0]  # Get the largest file\n",
    "\n",
    "            # Move the largest file back to the original folder\n",
    "            original_folder = os.path.dirname(os.path.dirname(largest_file))  # Go back two levels to the original folder\n",
    "            new_name = os.path.basename(largest_file)  # Keep the renamed file (e.g., DuplicateGroupX_Y.jpg)\n",
    "            new_path = os.path.join(original_folder, new_name)\n",
    "            shutil.move(largest_file, new_path)\n",
    "            print(f\"Moving largest file {largest_file} back to original folder: {new_path}\")\n",
    "\n",
    "            # Delete all smaller files\n",
    "            for file, _ in group_sorted_by_size[1:]:\n",
    "                try:\n",
    "                    print(f\"Deleting smaller file: {file}\")\n",
    "                    os.remove(file)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found during deletion: {file}. It might have been deleted already.\")\n",
    "    else:\n",
    "        print(\"No files were deleted.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hamming_threshold = 10  # Define the maximum Hamming distance to consider images as duplicates\n",
    "    # Change your directory to the one where you want to find and delete duplicate images\n",
    "    directory_path = r'C:/Users/YourUserName/Desktop/Food AI/images/Malaysian Dish/'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"Directory {directory_path} does not exist.\")\n",
    "    else:\n",
    "        # Find duplicates based on the Hamming distance threshold\n",
    "        duplicates = find_duplicates(directory_path, hamming_threshold)\n",
    "\n",
    "        # Move and rename duplicates to a separate folder within each food item folder\n",
    "        if duplicates:\n",
    "            grouped_new_paths = move_and_rename_duplicates(duplicates)\n",
    "            # Prompt user to delete smaller duplicates in the 'duplicates' folder\n",
    "            delete_smaller_duplicates_and_move_back(grouped_new_paths)\n",
    "        else:\n",
    "            print(\"No duplicates found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e9760-2811-4716-8fcf-d7be15c4c62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChromeImageScrape",
   "language": "python",
   "name": "chromeimagescrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
