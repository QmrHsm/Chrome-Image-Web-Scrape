{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e240b94-dec3-4755-8726-7eeec8a5080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import io\n",
    "from datetime import datetime as dt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "from urllib.parse import quote_plus\n",
    "import re\n",
    "\n",
    "# Download the driver from the ChromeDriver website for the relevant OS i.e. MAC, Windows, Debian, etc.\n",
    "PATH = r'C:/path/to/your/chromedriver.exe'\n",
    "service = Service(executable_path=PATH)\n",
    "\n",
    "# Initialize the WebDriver with the Service object\n",
    "wd = webdriver.Chrome(service=service)\n",
    "\n",
    "def get_images_from_google(wd, delay, max_images, url):\n",
    "    def scroll_down(wd):\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "    wd.get(url)\n",
    "\n",
    "    image_urls = set()\n",
    "    skips = 0\n",
    "    while len(image_urls) + skips < max_images:\n",
    "        scroll_down(wd)\n",
    "        thumbnails = wd.find_elements(By.CLASS_NAME, \"mNsIhb\")\n",
    "\n",
    "        for img in thumbnails[len(image_urls) + skips:max_images]:\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(delay)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            images = wd.find_elements(By.CLASS_NAME, \"sFlh5c\")\n",
    "            for image in images:\n",
    "                if image.get_attribute('src') in image_urls:\n",
    "                    max_images += 1\n",
    "                    skips += 1\n",
    "                    break\n",
    "\n",
    "                if image.get_attribute('src') and 'http' in image.get_attribute('src'):\n",
    "                    image_urls.add(image.get_attribute('src'))\n",
    "                    ##print(f\"Found {len(image_urls)}\")\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "def download_image(down_path, url, file_name, image_type='JPEG', verbose=True):\n",
    "    try:\n",
    "        time = dt.now()\n",
    "        curr_time = time.strftime('%H:%M:%S')\n",
    "        # Content of the image will be a url\n",
    "        img_content = requests.get(url).content\n",
    "        # Get the bytes IO of the image\n",
    "        img_file = io.BytesIO(img_content)\n",
    "        # Stores the file in memory and convert to image file using Pillow\n",
    "        image = Image.open(img_file)\n",
    "        file_pth = down_path + file_name\n",
    "\n",
    "        with open(file_pth, 'wb') as file:\n",
    "            image.save(file, image_type)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'The image: {file_pth} downloaded successfully at {curr_time}.')\n",
    "    except Exception as e:\n",
    "        print(f'Unable to download image from Google Photos due to\\n: {str(e)}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ask user for the list of foods they want to search, separated by commas\n",
    "    food_list = input(\"Enter the names of the foods you want to search for, separated by commas: \").split(',')\n",
    "    food_list = [food.strip() for food in food_list]  # Remove any leading/trailing whitespace\n",
    "\n",
    "    # Loop to ensure valid integer input for the maximum number of images to scrape\n",
    "    while True:\n",
    "        try:\n",
    "            max_images = int(input(\"Enter the maximum number of images to scrape for each food item: \"))\n",
    "            break  # Exit the loop if a valid integer is entered\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer for the number of images.\")\n",
    "\n",
    "    # Get the current date in the format YYYY-MM-DD\n",
    "    current_date = dt.now().strftime('%Y-%m-%d')\n",
    "    base_directory = f'C:/Users/YourUserName/Desktop/Food AI/images/Malaysian Dish/{current_date}/'\n",
    "\n",
    "    # Make the base directory for the current date if it doesn't exist\n",
    "    if not os.path.exists(base_directory):\n",
    "        print(f'Making base directory for the current date: {base_directory}')\n",
    "        os.makedirs(base_directory)\n",
    "\n",
    "    for food_name in food_list:\n",
    "        # URL encode the food name for the Google search URL\n",
    "        search_query = quote_plus(food_name)\n",
    "        google_url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
    "\n",
    "        # Directory to save the images, named after the food\n",
    "        sanitized_name = re.sub(r'[\\\\/:*?\"<>|]', '', food_name)\n",
    "        save_directory = os.path.join(base_directory, sanitized_name)\n",
    "        \n",
    "        # Make the directory for the specific food if it doesn't exist\n",
    "        if not os.path.exists(save_directory):\n",
    "            print(f'Making directory for {food_name}: {save_directory}')\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        # Scrape images from Google\n",
    "        urls = get_images_from_google(wd, delay=0.5, max_images=max_images, url=google_url)\n",
    "        \n",
    "        # Download the images\n",
    "        for i, url in enumerate(urls):\n",
    "            download_image(down_path=save_directory + '/', \n",
    "                           url=url, \n",
    "                           file_name=str(i+1) + '.jpg',\n",
    "                           verbose=True)\n",
    "\n",
    "    # Close the webdriver\n",
    "    wd.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d708f48-08a3-4b5f-9fd5-ab77abe3dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import imagehash\n",
    "\n",
    "def find_duplicates(directory_path, hamming_threshold):\n",
    "    image_hashes = {}  # Dictionary to store image hashes\n",
    "    duplicates = []  # List to store pairs of duplicate images\n",
    "\n",
    "    # Calculate image hashes for each image in the directory\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    image = Image.open(image_path)\n",
    "                    image_hash = imagehash.phash(image)\n",
    "                    \n",
    "                    # Check for duplicates using Hamming distance\n",
    "                    found_duplicate = False\n",
    "                    for existing_hash, existing_path in image_hashes.items():\n",
    "                        hamming_distance = image_hash - existing_hash\n",
    "                        if hamming_distance <= hamming_threshold:\n",
    "                            duplicates.append((existing_path, image_path))\n",
    "                            found_duplicate = True\n",
    "                            break\n",
    "                    \n",
    "                    # Only add to the dictionary if no duplicate was found\n",
    "                    if not found_duplicate:\n",
    "                        image_hashes[image_hash] = image_path\n",
    "\n",
    "                except UnidentifiedImageError:\n",
    "                        print(f\"Cannot identify image file: {image_path}. Deleting file.\")\n",
    "                        os.remove(image_path)  # Automatically delete the file\n",
    "\n",
    "\n",
    "    print(\"Duplicate detection result:\")\n",
    "    if duplicates:\n",
    "        for dup1, dup2 in duplicates:\n",
    "            # Calculate the Hamming distance\n",
    "            hash1 = imagehash.phash(Image.open(dup1))\n",
    "            hash2 = imagehash.phash(Image.open(dup2))\n",
    "            distance = hash1 - hash2\n",
    "            similarity = calculate_similarity(distance, hamming_threshold)\n",
    "            print(f\"Duplicate found: {dup1} and {dup2} with Hamming distance: {distance} and similarity: {similarity}%\")\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "def calculate_similarity(hamming_distance, max_distance):\n",
    "    similarity_percentage = (1 - (hamming_distance / max_distance)) * 100\n",
    "    return round(similarity_percentage, 2)\n",
    "\n",
    "def delete_smallest_duplicates(directory_path, hamming_threshold):\n",
    "    duplicates = find_duplicates(directory_path, hamming_threshold)\n",
    "\n",
    "    print(f\"Note: It is preferable to check if the image similarity is below 50% before deciding to delete.\")\n",
    "    choice = input(f\"Do you want to delete all the duplicate image:? (y/any key to skip) \").strip().lower()\n",
    "\n",
    "    if choice == 'y' and choice == 'Y':\n",
    "        for dup1, dup2 in duplicates:\n",
    "            size1 = os.path.getsize(dup1)\n",
    "            size2 = os.path.getsize(dup2)\n",
    "            if size1 < size2:\n",
    "                print(f\"Deleting smaller file: {dup1}\")\n",
    "                os.remove(dup1)\n",
    "            else:\n",
    "                print(f\"Deleting smaller file: {dup2}\")\n",
    "                os.remove(dup2)\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping deletion for all possible duplicate images.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hamming_threshold = 10  # Define the maximum Hamming distance to consider images as duplicates\n",
    "    # Change your directory to the one where you want to find and delete duplicate images\n",
    "    directory_path = r'C:/Users/YourUserName/Desktop/Food AI/images/Malaysian Dish/'\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"Directory {directory_path} does not exist.\")\n",
    "    else:\n",
    "        # Find and delete duplicates based on the Hamming distance threshold\n",
    "        delete_smallest_duplicates(directory_path, hamming_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e9760-2811-4716-8fcf-d7be15c4c62e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChromeImageScrape",
   "language": "python",
   "name": "chromeimagescrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
