{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489006f4-ad35-4afd-91fc-4394fe9b197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping\n",
    "from PIL import Image\n",
    "import os\n",
    "import requests\n",
    "import io\n",
    "from urllib.parse import quote_plus\n",
    "import re\n",
    "import ctypes\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, ElementClickInterceptedException, ElementNotInteractableException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Prevent sleep mode while the script is running\n",
    "ctypes.windll.kernel32.SetThreadExecutionState(0x80000002)\n",
    "\n",
    "# Download the driver from the ChromeDriver website for the relevant OS i.e. MAC, Windows, etc.\n",
    "PATH = r'C:/path/to/your/chromedriver.exe'\n",
    "service = Service(executable_path=PATH)\n",
    "\n",
    "# Initialize the WebDriver with the Service object\n",
    "wd = webdriver.Chrome(service=service)\n",
    "\n",
    "def get_images_from_google(wd, delay, max_images, url):\n",
    "    def scroll_down(wd):\n",
    "        wd.execute_script(\"window.scrollBy(0, 200);\")  # Scroll slightly to move past suggestions\n",
    "        time.sleep(1)  # Allow time for suggestions to be cleared\n",
    "\n",
    "    wd.get(url)\n",
    "    scroll_down(wd)  # Scroll immediately after the search page loads\n",
    "\n",
    "    image_urls = set()\n",
    "    skips = 0\n",
    "    while len(image_urls) + skips < max_images:\n",
    "        # Find the image thumbnails on the page\n",
    "        thumbnails = wd.find_elements(By.CLASS_NAME, \"mNsIhb\")\n",
    "\n",
    "        for img in thumbnails[len(image_urls) + skips:max_images]:\n",
    "            try:\n",
    "                # Ensure the thumbnail is clickable before clicking\n",
    "                WebDriverWait(wd, 10).until(EC.element_to_be_clickable((By.CLASS_NAME, \"mNsIhb\")))\n",
    "\n",
    "                # Scroll the image into view to ensure it's interactable\n",
    "                wd.execute_script(\"arguments[0].scrollIntoView(true);\", img)\n",
    "\n",
    "                img.click()  # Click the image\n",
    "                time.sleep(delay)\n",
    "\n",
    "            except (ElementClickInterceptedException, TimeoutException, ElementNotInteractableException) as e:\n",
    "                # Log or print the error and skip the problematic element\n",
    "                print(f\"Error clicking on thumbnail: {e}\")\n",
    "                skips += 1\n",
    "                continue\n",
    "\n",
    "            # Now grab the large image\n",
    "            images = wd.find_elements(By.CSS_SELECTOR, \"img.sFlh5c:not(.GKS7s):not(.bqW4cb)\")\n",
    "            for image in images:\n",
    "                if image.get_attribute('src') in image_urls:\n",
    "                    max_images += 1  # Adjust max_images to continue gathering more unique images\n",
    "                    skips += 1\n",
    "                    break\n",
    "\n",
    "                if image.get_attribute('src') and 'http' in image.get_attribute('src'):\n",
    "                    image_urls.add(image.get_attribute('src'))\n",
    "                    \n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "        # Break the loop if no new thumbnails are found\n",
    "        if len(thumbnails) == len(image_urls) + skips:\n",
    "            print(f\"No more thumbnails found. Proceed....\")\n",
    "            break\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "\n",
    "def download_image(down_path, url, file_name, image_type='JPEG', verbose=True):\n",
    "    try:\n",
    "        time = dt.now()\n",
    "        curr_time = time.strftime('%H:%M:%S')\n",
    "        # Content of the image will be a url\n",
    "        img_content = requests.get(url).content\n",
    "        # Get the bytes IO of the image\n",
    "        img_file = io.BytesIO(img_content)\n",
    "        # Stores the file in memory and convert to image file using Pillow\n",
    "        image = Image.open(img_file)\n",
    "\n",
    "        # If image is in WebP format, convert it to JPEG or PNG\n",
    "        if image.format == 'WEBP':\n",
    "            print(f\"Converting WebP image to {image_type} format...\")\n",
    "            file_name = file_name.replace('.jpg', '.webp')  # Change extension if you prefer keeping WebP format\n",
    "            file_pth = down_path + file_name\n",
    "            image = image.convert(\"RGB\")  # Convert WebP to RGB before saving as JPEG/PNG\n",
    "\n",
    "            # Save as JPEG/PNG depending on the user's choice\n",
    "            image.save(file_pth, image_type)\n",
    "        else:\n",
    "            # Save the image normally for non-WebP formats\n",
    "            file_pth = down_path + file_name\n",
    "            image.save(file_pth, image_type)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'The image: {file_pth} downloaded successfully at {curr_time}.')\n",
    "    except Exception as e:\n",
    "        print(f'Unable to download image from Google Photos')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Ask user for the list of foods they want to search, separated by commas\n",
    "    food_list = input(\"Enter the names of the foods you want to search for, separated by commas: \").split(',')\n",
    "    food_list = [food.strip() for food in food_list]  # Remove any leading/trailing whitespace\n",
    "\n",
    "    # Loop to ensure valid integer input for the maximum number of images to scrape\n",
    "    while True:\n",
    "        try:\n",
    "            max_images = int(input(\"Enter the maximum number of images to scrape for each food item: \"))\n",
    "            break  # Exit the loop if a valid integer is entered\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid integer for the number of images.\")\n",
    "\n",
    "    # Get the current date in the format YYYY-MM-DD\n",
    "    current_date = dt.now().strftime('%Y-%m-%d')\n",
    "    base_directory = f'C:/Users/YourUserName/Desktop/Food AI/images/Malaysian Dish/{current_date}/'\n",
    "\n",
    "    # Make the base directory for the current date if it doesn't exist\n",
    "    if not os.path.exists(base_directory):\n",
    "        print(f'Making base directory for the current date: {base_directory}')\n",
    "        os.makedirs(base_directory)\n",
    "\n",
    "    for food_name in food_list:\n",
    "        # URL encode the food name for the Google search URL\n",
    "        search_query = quote_plus(food_name)\n",
    "        google_url = f\"https://www.google.com/search?q={search_query}&tbm=isch\"\n",
    "\n",
    "        # Directory to save the images, named after the food\n",
    "        sanitized_name = re.sub(r'[\\\\/:*?\"<>|]', '', food_name)\n",
    "        save_directory = os.path.join(base_directory, sanitized_name)\n",
    "\n",
    "        # Make the directory for the specific food if it doesn't exist\n",
    "        if not os.path.exists(save_directory):\n",
    "            print(f'Making directory for {food_name}: {save_directory}')\n",
    "            os.makedirs(save_directory)\n",
    "\n",
    "        # Scrape images from Google\n",
    "        urls = get_images_from_google(wd, delay=0.5, max_images=max_images, url=google_url)\n",
    "\n",
    "        # Download the images\n",
    "        for i, url in enumerate(urls):\n",
    "            download_image(down_path=save_directory + '/', \n",
    "                           url=url, \n",
    "                           file_name=f\"{sanitized_name}_\" + str(i+1) + '.jpg',\n",
    "                           verbose=True)\n",
    "\n",
    "    # Close the webdriver\n",
    "    wd.quit()\n",
    "\n",
    "    # Allow sleep mode again after script execution\n",
    "    ctypes.windll.kernel32.SetThreadExecutionState(0x80000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d708f48-08a3-4b5f-9fd5-ab77abe3dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate Detection\n",
    "\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import imagehash\n",
    "import shutil\n",
    "\n",
    "def find_duplicates(directory_path, hamming_threshold):\n",
    "    image_hashes = {}  # Dictionary to store image hashes\n",
    "    duplicates = []  # List to store groups of duplicate images\n",
    "\n",
    "    # Calculate image hashes for each image in the directory\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        # Ignore the 'duplicates' folder\n",
    "        if 'duplicates' in root or 'collage_images' in root or 'text_detected' in root:\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    image = Image.open(image_path)\n",
    "                    image_hash = imagehash.phash(image)\n",
    "\n",
    "                    # Check for duplicates using Hamming distance\n",
    "                    found_duplicate = False\n",
    "                    for existing_hash, existing_paths in image_hashes.items():\n",
    "                        hamming_distance = image_hash - existing_hash\n",
    "                        if hamming_distance <= hamming_threshold:\n",
    "                            existing_paths.append(image_path)  # Add to existing group of duplicates\n",
    "                            found_duplicate = True\n",
    "                            break\n",
    "\n",
    "                    # Only add to the dictionary if no duplicate was found\n",
    "                    if not found_duplicate:\n",
    "                        image_hashes[image_hash] = [image_path]\n",
    "\n",
    "                except UnidentifiedImageError:\n",
    "                    print(f\"Cannot identify image file: {image_path}. Deleting file.\")\n",
    "                    os.remove(image_path)  # Automatically delete the file\n",
    "\n",
    "    print(\"Duplicate detection result:\")\n",
    "    duplicates = [paths for paths in image_hashes.values() if len(paths) > 1]\n",
    "    return duplicates\n",
    "\n",
    "def calculate_similarity(hamming_distance, max_distance):\n",
    "    similarity_percentage = (1 - (hamming_distance / max_distance)) * 100\n",
    "    return round(similarity_percentage, 2)\n",
    "\n",
    "def move_and_rename_duplicates(duplicates):\n",
    "    pair_counter = {}  # To track pair counts for each food folder\n",
    "    grouped_new_paths = []  # Store new paths grouped by duplicate groups\n",
    "\n",
    "    for group in duplicates:\n",
    "        food_folder = os.path.dirname(group[0])\n",
    "        food_name = os.path.basename(food_folder)  # Extract the food folder name\n",
    "        duplicates_folder = os.path.join(food_folder, 'duplicates')\n",
    "\n",
    "        # Ensure the duplicates folder exists\n",
    "        if not os.path.exists(duplicates_folder):\n",
    "            os.makedirs(duplicates_folder)\n",
    "            print(f\"Created 'duplicates' folder at: {duplicates_folder}\")\n",
    "\n",
    "        # Initialize the pair counter for this food item folder\n",
    "        if food_name not in pair_counter:\n",
    "            pair_counter[food_name] = 1\n",
    "\n",
    "        new_group_paths = []  # To store the paths of the current group\n",
    "\n",
    "        # Move and rename each duplicate in the group\n",
    "        pair_number = pair_counter[food_name]\n",
    "        for idx, dup in enumerate(group):\n",
    "            file_name = f\"{food_name}_DuplicateGroup{pair_number}_{idx + 1}{os.path.splitext(dup)[1]}\"\n",
    "            dest_path = os.path.join(duplicates_folder, file_name)\n",
    "            shutil.move(dup, dest_path)\n",
    "            print(f\"Moved and renamed {dup} to {file_name}\")\n",
    "            new_group_paths.append(dest_path)  # Add new path to the current group\n",
    "\n",
    "        # Store the grouped paths of this duplicate group\n",
    "        grouped_new_paths.append(new_group_paths)\n",
    "\n",
    "        # Increment the pair number for the next group in the same folder\n",
    "        pair_counter[food_name] += 1\n",
    "\n",
    "    return grouped_new_paths\n",
    "\n",
    "def delete_smaller_duplicates_and_move_back(grouped_new_paths):\n",
    "    choice = input(f\"Do you want to delete all duplicates except the largest file? (y/n): \").strip().lower()\n",
    "    if choice == 'y' or choice == 'Y':\n",
    "        for group in grouped_new_paths:\n",
    "            # Sort the duplicates by file size\n",
    "            group_sorted_by_size = []\n",
    "            for file in group:\n",
    "                try:\n",
    "                    # Attempt to get the file size and add to sorted list\n",
    "                    size = os.path.getsize(file)\n",
    "                    group_sorted_by_size.append((file, size))\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {file}. Skipping.\")\n",
    "            \n",
    "            if not group_sorted_by_size:\n",
    "                # If no valid files are found, skip the group\n",
    "                print(\"No valid files found in this group. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Sort by file size in descending order\n",
    "            group_sorted_by_size.sort(key=lambda x: x[1], reverse=True)\n",
    "            largest_file = group_sorted_by_size[0][0]  # Get the largest file\n",
    "\n",
    "            # Move the largest file back to the original folder\n",
    "            original_folder = os.path.dirname(os.path.dirname(largest_file))  # Go back two levels to the original folder\n",
    "            new_name = os.path.basename(largest_file)  # Keep the renamed file (e.g., DuplicateGroupX_Y.jpg)\n",
    "            new_path = os.path.join(original_folder, new_name)\n",
    "            shutil.move(largest_file, new_path)\n",
    "            print(f\"Moving largest file {largest_file} back to original folder: {new_path}\")\n",
    "\n",
    "            # Delete all smaller files\n",
    "            for file, _ in group_sorted_by_size[1:]:\n",
    "                try:\n",
    "                    print(f\"Deleting smaller file: {file}\")\n",
    "                    os.remove(file)\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found during deletion: {file}. It might have been deleted already.\")\n",
    "    else:\n",
    "        print(\"No files were deleted.\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hamming_threshold = 10  # Define the maximum Hamming distance to consider images as duplicates\n",
    "    # Change your directory to the one where you want to find and delete duplicate images\n",
    "    directory_path = r'C:/Users/YourUserName/Desktop/Food AI/images/Malaysian Dish/'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        print(f\"Directory {directory_path} does not exist.\")\n",
    "    else:\n",
    "        # Find duplicates based on the Hamming distance threshold\n",
    "        duplicates = find_duplicates(directory_path, hamming_threshold)\n",
    "\n",
    "        # Move and rename duplicates to a separate folder within each food item folder\n",
    "        if duplicates:\n",
    "            grouped_new_paths = move_and_rename_duplicates(duplicates)\n",
    "            # Prompt user to delete smaller duplicates in the 'duplicates' folder\n",
    "            delete_smaller_duplicates_and_move_back(grouped_new_paths)\n",
    "        else:\n",
    "            print(\"No duplicates found.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e9760-2811-4716-8fcf-d7be15c4c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count remaining image in every food folder\n",
    "import os\n",
    "\n",
    "def count_non_duplicate_images(directory_path):\n",
    "    # Iterate over the date folders first\n",
    "    for root, dirs, _ in os.walk(directory_path):\n",
    "        for date_folder in dirs:\n",
    "            date_folder_path = os.path.join(root, date_folder)\n",
    "            # Go one level deeper to get the food item folders\n",
    "            for food_folder in os.listdir(date_folder_path):\n",
    "                food_folder_path = os.path.join(date_folder_path, food_folder)\n",
    "                if os.path.isdir(food_folder_path) and 'duplicates' not in food_folder_path and 'text_detected' not in food_folder_path:\n",
    "                    # Count images in the food item folder (excluding the 'duplicates' folder)\n",
    "                    non_duplicate_images = [file for file in os.listdir(food_folder_path)\n",
    "                                            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif'))]\n",
    "                    print(f\"({date_folder}) Folder: {food_folder}: {len(non_duplicate_images)}\")\n",
    "                    \n",
    "            #print('\\n')\n",
    "\n",
    "# Usage example\n",
    "directory_path = r'C:/Users/YourUserName/Desktop/Food AI/images/Malaysian Dish/'\n",
    "count_non_duplicate_images(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd840b-d082-4dc3-aeaa-00eccb45a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting text in images\n",
    "#Still not perform well \n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import easyocr\n",
    "#import pytesseract\n",
    "\n",
    "def preprocess_image(image_path, target_size=(800, 800)):\n",
    "    # Load image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Skip resizing for smaller images\n",
    "    if img.shape[0] > target_size[0] or img.shape[1] > target_size[1]:\n",
    "        img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Optional: Apply histogram equalization for contrast enhancement only for low-contrast images\n",
    "    if np.mean(gray) < 100:  # Apply only if the image is dark/low contrast\n",
    "        gray = cv2.equalizeHist(gray)\n",
    "    \n",
    "    # Apply a global threshold instead of adaptive thresholding for simpler images\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Skip sharpening unless necessary (e.g., for blurry images)\n",
    "    if cv2.Laplacian(thresh, cv2.CV_64F).var() < 100:  # Apply sharpening only if the image is blurry\n",
    "        kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "        thresh = cv2.filter2D(thresh, -1, kernel)\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "def detect_text_in_images(directory_path):\n",
    "    for root, _, files in os.walk(directory_path):\n",
    "        # Skip 'text_detected' folder and other excluded folders\n",
    "        if 'duplicates' in root or 'collage_images' in root or 'text_detected' in root:\n",
    "            continue\n",
    "\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                image_path = os.path.join(root, file)\n",
    "\n",
    "                text_detected_folder = os.path.join(root, 'text_detected')\n",
    "                if not os.path.exists(text_detected_folder):\n",
    "                    os.makedirs(text_detected_folder)\n",
    "\n",
    "                try:\n",
    "                    # Preprocess the image before OCR\n",
    "                    processed_img = preprocess_image(image_path)\n",
    "                    \n",
    "                    # Perform text detection using pytesseract on preprocessed image\n",
    "\n",
    "                    reader = easyocr.Reader(['en','ms'], gpu=True)\n",
    "                    result = reader.readtext(processed_img)\n",
    "                    #text = pytesseract.image_to_string(processed_img)\n",
    "\n",
    "                    # If text is detected, move the image to the 'text_detected' folder\n",
    "                    for (bbox, text, prob) in result:\n",
    "                        if text.strip():  # Check if detected text is not empty\n",
    "                            dest_path = os.path.join(text_detected_folder, file)\n",
    "                            shutil.move(image_path, dest_path)\n",
    "                            print(f\"Moved image with text: {file} to 'text_detected' folder\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    directory_path = r'C:/Users/YourUserName/Desktop/Food AI/images/Malaysian Dish/'\n",
    "    detect_text_in_images(directory_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChromeImageScrape",
   "language": "python",
   "name": "chromeimagescrape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
